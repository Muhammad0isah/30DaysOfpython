{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Day 19 (File Handling)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercises: Level 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama Speech \n",
      "Number of words:  2400\n",
      "Number of lines:  66\n",
      "\n",
      "Michelle Obama Speech\n",
      "Number of words:  2204\n",
      "Number of lines:  83\n",
      "\n",
      "Donald Speech \n",
      "Number of words:  1259\n",
      "Number of lines:  48\n",
      "\n",
      "Melania Trump Speech\n",
      "Number of words:  1375\n",
      "Number of lines:  33\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "def counter(file_name):\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip(os.linesep)\n",
    "            words_list = line.split()\n",
    "            num_lines += 1\n",
    "            num_words += len(words_list)\n",
    "\n",
    "    print(\"Number of words: \", num_words)\n",
    "    print(\"Number of lines: \", num_lines)\n",
    "\n",
    "# Q1(a) Read obama_speech.txt\n",
    "print(\"Obama Speech \")\n",
    "counter('./data/obama_speech.txt')\n",
    "\n",
    "# Q1(b) Read michelle_obama_speech.txt\n",
    "print(\"\\nMichelle Obama Speech\")\n",
    "counter(\"./data/michelle_obama_speech.txt\")\n",
    "\n",
    "# Q1(c) Read donald_speech.txt\n",
    "print(\"\\nDonald Speech \")\n",
    "counter(\"./data/donald_speech.txt\")\n",
    "\n",
    "# Q1(d) Read melina_trump_speech.txt\n",
    "print(\"\\nMelania Trump Speech\")\n",
    "counter(\"./data/melina_trump_speech.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 most spoken languages from the data\n",
      "[(91, 'English'), (45, 'French'), (25, 'Arabic'), (24, 'Spanish'), (9, 'Portuguese'), (9, 'Russian'), (8, 'Dutch'), (7, 'German'), (5, 'Chinese'), (4, 'Serbian')]\n",
      "top 4 most spoken languages from the data\n",
      "[(91, 'English'), (45, 'French'), (25, 'Arabic'), (24, 'Spanish')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def most_spoken_languages(filename,value):\n",
    "\n",
    "    languages_lst = []\n",
    "    for i in json.load(open(filename,encoding=\"UTF8\")):\n",
    "        languages_lst.extend(i['languages'])\n",
    "\n",
    "    #print('Total number of languages is',len(set(languages_lst)))\n",
    "    languages_dict = {}\n",
    "    for j in languages_lst:\n",
    "        if j in languages_dict:\n",
    "            languages_dict[j] += 1\n",
    "        else:\n",
    "            languages_dict[j] = 1\n",
    "    # sort languages dictionary by value to get top language\n",
    "    sorted_language_dict= dict(sorted(languages_dict.items(),key=lambda x:x[1],reverse=True))\n",
    "\n",
    "    # get top languages list by passing value\n",
    "    top_language = list(sorted_language_dict.items())[:value]\n",
    "    print(\"top {} most spoken languages from the data\".format(value))\n",
    "    return [(i[1],i[0]) for  i in top_language]\n",
    "\n",
    "print(most_spoken_languages('./data/countries_data.json',10))\n",
    "print(most_spoken_languages('./data/countries_data.json',4))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 most populated countries in the world\n",
      "[{'country': 'China', 'populations': 1377422166}, {'country': 'India', 'populations': 1295210000}, {'country': 'United States of America', 'populations': 323947000}, {'country': 'Indonesia', 'populations': 258705000}, {'country': 'Brazil', 'populations': 206135893}, {'country': 'Pakistan', 'populations': 194125062}, {'country': 'Nigeria', 'populations': 186988000}, {'country': 'Bangladesh', 'populations': 161006790}, {'country': 'Russian Federation', 'populations': 146599183}, {'country': 'Japan', 'populations': 126960000}]\n",
      "top 4 most populated countries in the world\n",
      "[{'country': 'China', 'populations': 1377422166}, {'country': 'India', 'populations': 1295210000}, {'country': 'United States of America', 'populations': 323947000}, {'country': 'Indonesia', 'populations': 258705000}]\n"
     ]
    }
   ],
   "source": [
    "def most_populated_countries(filename,value):\n",
    "    populations = {}\n",
    "    for i in json.load(open(filename,encoding=\"UTF8\")):\n",
    "        populations[i[\"name\"]] = i[\"population\"]\n",
    "    sorted_populations_dict= dict(sorted(populations.items(),key=lambda x:x[1],reverse=True))\n",
    "    top_most_populated_countries = []\n",
    "    for l in list(sorted_populations_dict.items())[:value]:\n",
    "        top_most_populated_countries.append({'country':l[0],'populations':l[1]})\n",
    "\n",
    "    print(\"top {} most populated countries in the world\".format(value))\n",
    "    return top_most_populated_countries\n",
    "\n",
    "print(most_populated_countries('./data/countries_data.json',10))\n",
    "print(most_populated_countries('./data/countries_data.json',4))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercises: Level 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email List:\n",
      "['jleasia@umich.edu', 'mbreuker@loi.nl', 'a.fish@lancaster.ac.uk', 'rjlowe@iupui.edu', 'wang58@iupui.edu', 'jbush@rsmart.com', 'jimeng@umich.edu', 'arwhyte@umich.edu', 'kimsooil@bu.edu', 'jzaremba@unicon.net', 'josrodri@iupui.edu', 'source@collab.sakaiproject.org', 'mmmay@indiana.edu', 'stephen.marquard@uct.ac.za', 'antranig@caret.cam.ac.uk', 'csev@umich.edu', 'bkirschn@umich.edu', 'stuart.freeman@et.gatech.edu', 'aaronz@vt.edu', 'ggolden@umich.edu', 'nuno@ufp.pt', 'zach.thomas@txstate.edu', 'thoppaymallika@fhda.edu', 'jholtzman@berkeley.edu', 'dsobiera@indiana.edu', 'ktsao@stanford.edu', 'sgithens@caret.cam.ac.uk', 'cwen@iupui.edu', 'wagnermr@iupui.edu', 'ssmail@indiana.edu', 'joshua.ryan@asu.edu', 'chmaurer@iupui.edu', 'hu2@iupui.edu', 'louis@media.berkeley.edu', 'lance@indiana.edu', 'gsilver@umich.edu', 'david.horwitz@uct.ac.za', 'ian@caret.cam.ac.uk', 'john.ellis@rsmart.com', 'gjthomas@iupui.edu', 'gbhatnag@umich.edu', 'zqian@umich.edu', 'gopal.ramasammycook@gmail.com', 'colin.clark@utoronto.ca', 'jlrenfro@ucdavis.edu', 'ajpoland@iupui.edu', 'dlhaines@umich.edu', 'ray@media.berkeley.edu', 'eli@media.berkeley.edu', 'bahollad@indiana.edu', 'knoop@umich.edu', 'ostermmg@whitman.edu', 'tnguyen@iupui.edu']\n"
     ]
    }
   ],
   "source": [
    "def extract_emails(fname):\n",
    "    word_list = []\n",
    "    with open(fname, \"r\") as file:\n",
    "        for line in file:\n",
    "            for word in line.split():\n",
    "                word_list.append(word)\n",
    "    # change each email address in the list  to unique using set data type\n",
    "    word_list = list(set(word_list))\n",
    "\n",
    "    email_list = []\n",
    "    for word in word_list:\n",
    "        if re.fullmatch(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", word):\n",
    "            email_list.append(word)\n",
    "    return email_list\n",
    "\n",
    "print('Email List:')\n",
    "print(extract_emails(\"data\\email_exchanges_big.txt\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(61, 'the'), (53, 'and'), (40, 'will'), (38, 'of'), (32, 'to'), (30, 'our'), (26, 'we'), (20, 'is'), (15, 'We'), (14, 'â€“')]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def find_most_common_words(filename, value):\n",
    "    text = open(filename).read()\n",
    "    split_text = text.split()\n",
    "    my_counter = [(i[1], i[0]) for i in Counter(split_text).most_common()]\n",
    "\n",
    "    return my_counter[:value]\n",
    "\n",
    "print(find_most_common_words('data\\donald_speech.txt', 10))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama Speech\n",
      "[(120, 'the'), (107, 'and'), (81, 'of'), (66, 'to'), (58, 'our'), (50, 'we'), (48, 'a'), (47, 'that'), (36, 'is'), (23, '-')]\n",
      "Michelle Speech\n",
      "[(83, 'to'), (80, 'and'), (78, 'the'), (46, 'of'), (41, 'â€”'), (41, 'a'), (40, 'that'), (36, 'in'), (27, 'our'), (26, 'for')]\n",
      "Donald Speech\n",
      "[(61, 'the'), (53, 'and'), (40, 'will'), (38, 'of'), (32, 'to'), (30, 'our'), (26, 'we'), (20, 'is'), (15, 'We'), (14, 'â€“')]\n",
      "Melina Speech\n",
      "[(73, 'and'), (54, 'to'), (48, 'the'), (28, 'I'), (28, 'is'), (27, 'for'), (25, 'of'), (22, 'a'), (19, 'that'), (17, 'Donald')]\n"
     ]
    }
   ],
   "source": [
    "# Q6(a)\n",
    "print(\"Obama Speech\")\n",
    "print(find_most_common_words('./data/obama_speech.txt',10))\n",
    "\n",
    "# Q6(b)\n",
    "print(\"Michelle Speech\")\n",
    "print(find_most_common_words('./data/michelle_obama_speech.txt',10))\n",
    "\n",
    "# Q6(c)\n",
    "print(\"Donald Speech\")\n",
    "print(find_most_common_words('./data/donald_speech.txt',10))\n",
    "\n",
    "# Q6(d)\n",
    "print(\"Melina Speech\")\n",
    "print(find_most_common_words('./data/melina_trump_speech.txt',10))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q7"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data\\michelle_obama_speech.txt :\n",
      "590 distinct words\n",
      "File data\\melina_trump_speech.txt :\n",
      "414 distinct words\n",
      "The distance between the documents is:  63.23 (degrees)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from data.stop_words import stop_words\n",
    "import math\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\[.*?]\", \"\", text)\n",
    "    text = re.sub(\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(\"<.*?>+\", \"\", text)\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_support_words(text):\n",
    "    return [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "\n",
    "def read_file(fname):\n",
    "    try:\n",
    "        with open(fname, \"r\", encoding=\"UTF8\") as f:\n",
    "            data = remove_support_words(clean_text(f.read()))\n",
    "        return data\n",
    "    except IOError:\n",
    "        print(\"Error opening or reading input file: \", fname)\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "translation_table = str.maketrans(\n",
    "    string.punctuation + string.ascii_uppercase,\n",
    "    \" \" * len(string.punctuation) + string.ascii_lowercase,\n",
    ")\n",
    "\n",
    "def count_frequency(word_list):\n",
    "    D = {}\n",
    "\n",
    "    for new_word in word_list:\n",
    "\n",
    "        if new_word in D:\n",
    "            D[new_word] = D[new_word] + 1\n",
    "\n",
    "        else:\n",
    "            D[new_word] = 1\n",
    "\n",
    "    return D\n",
    "\n",
    "\n",
    "def word_frequencies_for_file(filename):\n",
    "    word_list = read_file(filename)\n",
    "    freq_mapping = count_frequency(word_list)\n",
    "\n",
    "    print(\n",
    "        \"File\",\n",
    "        filename,\n",
    "        \":\",\n",
    "    )\n",
    "    print(len(freq_mapping), \"distinct words\")\n",
    "\n",
    "    return freq_mapping\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "def dotProduct(D1, D2):\n",
    "    Sum = 0.0\n",
    "\n",
    "    for key in D1:\n",
    "\n",
    "        if key in D2:\n",
    "            Sum += D1[key] * D2[key]\n",
    "\n",
    "    return Sum\n",
    "\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "def vector_angle(D1, D2):\n",
    "    numerator = dotProduct(D1, D2)\n",
    "    denominator = math.sqrt(dotProduct(D1, D1) * dotProduct(D2, D2))\n",
    "\n",
    "    return math.acos(numerator / denominator)\n",
    "\n",
    "\n",
    "def documentSimilarity(filename_1, filename_2):\n",
    "    sorted_word_list_1 = word_frequencies_for_file(filename_1)\n",
    "    sorted_word_list_2 = word_frequencies_for_file(filename_2)\n",
    "    distance = (vector_angle(sorted_word_list_1, sorted_word_list_2) * 180) / math.pi\n",
    "\n",
    "    print(\"The distance between the documents is: % 0.2f (degrees)\" % distance)\n",
    "\n",
    "documentSimilarity(\"data\\michelle_obama_speech.txt\", \"data\\melina_trump_speech.txt\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(762, 'the'), (549, 'I'), (539, 'and'), (522, 'to'), (485, 'of'), (453, 'a'), (330, 'in'), (322, 'is'), (310, 'my'), (274, 'with')]\n"
     ]
    }
   ],
   "source": [
    "print(find_most_common_words('./data/romeo_and_juliet.txt',10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python or python = 179\n",
      "JavaScripts,Javascript or javascripts = 184\n",
      "Java and not JavaScripts = 222\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def hacker_count(fname):\n",
    "    csvFile = csv.reader(open(fname, mode=\"r\"))\n",
    "    count_a = 0\n",
    "    count_b = 0\n",
    "    count_c = 0\n",
    "    for lines in csvFile:\n",
    "        plain_text_line = \" \".join(lines)\n",
    "        if \"python\" in plain_text_line or \"Python\" in plain_text_line:\n",
    "            count_a += 1\n",
    "        if (\n",
    "                \"JavaScript\" in plain_text_line\n",
    "                or \"Javascript\" in plain_text_line\n",
    "                or \"javascript\" in plain_text_line\n",
    "        ):\n",
    "            count_b += 1\n",
    "        if not (not (\"Java\" in plain_text_line) or \"Javascript\" in plain_text_line):\n",
    "            count_c += 1\n",
    "    print('Python or python = {}\\nJavaScripts,Javascript or javascripts = {}\\nJava and not JavaScripts = {}'.format(count_a, count_b, count_c))\n",
    "\n",
    "hacker_count('data\\hacker_news.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
